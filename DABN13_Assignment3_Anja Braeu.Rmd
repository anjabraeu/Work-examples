---
title: "Assignment 3"
author: "Anja Braeu"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Preamble: Prostate specific antigen
Prostate specific antigen (PSA) might be know to you in the context of PSA tests, a widely used albeit somewhat controversial method to detect prostate cancer in middle-aged men. The dataset we are using in this lab contains measures of PSA levels in men shortly before they received prostate cancer treatment in the form of a surgical removal of their prostate. Additional variables are measures associated with the existing prostate cancer (log cancer volume Gleason score, percent of Gleason scores 4 or 5, seminal vesicle invasion, capsular penetration) as well as cancer-unrelated variables that are suspected to affect PSA levels (age, amount of benign prostatic hyperplasia, log prostate weight). 

## Part one: Mechanics
In this initial part, we are going to manually conduct the grid search over tuning parameter values that glmnet automatically provides us with when we fit the lasso or ridge regression. We are going to find the optimal tuning parameter for ridge regression since this allows us even to program the ridge regression estimator manually.
The goal with this part is that you by building the functions on your own will gain a greater understanding in what is presented in the lectures, and hopefully a deeper understanding.

Before conducting the first task, let us load some data.
```{r }
library(dplyr)
prostate <- read.table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data')
train_index  = prostate$train==T
prostate <- prostate %>% dplyr::select(-train)
prostate.train <- prostate[train_index,]
```

### Task 1a)
First we are going to demean all variables. In addition to that, we also want to standardize the sample variance of all predictors to one. 
We will use `preProcess()` function in the `caret` library. The reason is that we want to store the parameter used for scaling. Use the `scaled.param` to scale (demean and standardize) the training data.  Read `help(preProcess)` on how to demean and scale the data.
```{r }
library(caret)
scaled.param <- preProcess(prostate.train, method = c("center", "scale"))
prostate.train <- predict(scaled.param, prostate.train)
```
### Task 1b)
Since we are estimating ridge regression manually in this part, we need to make sure that the features of our dataset are transformed into numerical variables. Use the `model.matrix()` command to create such a matrix,`X`, of *all* predictors. Here, the intercept should not be included in the `X`  since the data is demeaned (centered). the data in the next step. Use a suitable specification for this purpose when expressing the formula in `model.matrix()'
Additionally, extract our outcome of interest from the data into a vector `y`.
```{r}
X <- model.matrix(lpsa~lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45 -1, data=prostate.train)
y <- prostate.train$lpsa
```
### Task 1c)
Demeaning all variables (`X` and `y`) has the convenient consequence that we do not need to include an intercept into our model. This is also very desirable in our manual implementation of ridge regression since we do not want to regularize the intercept. But WHY is it nonsensical to regularize the intercept? Please explain!
Additionally, please explain why it is most often a good idea to standardize the variance of all predictors to one.
``` {r}
intercept_regul1c <- "If we would regularize/penalize the intercept, the procedure would depend on the origin that was chosen for Y. To illustrate: if we have an unpenalized intercept term, adding a constant c to all 'y_i', will lead to '\beta_0' increasing by c and accordingly the predicted values 'hat{y_i}' also increase by c. Penalizing (shrinking) the intercept however, will not lead to the same shift of '\beta_0' by c (see Friedman et al. 2009, p.64). Also, the penalty encourages '\beta_0' to be small. Yet, the magnitude of the intercept does not really matter. Thus, intuitively, it does not make sense to shrink the intercept just because we have a very flexible model. To deal with this issue, the intercept can (1) be not penalized or (2) not be accounted for (if we center the data)." 
standardize_varc <- "The ridge solutions are not equivariant under scaling of the inputs, meaning that if we for example scale an input X, the estimators cannot simply be scaled by the inverse of the scaling (as it is the case for OLS). If the predictors don't have the same scale, they would contribute differently to the penalized terms (i.e. have smaller/bigger coefficients) and the shrinking would not be fair as ridge minimizes the sum of squares of all coefficients. To avoid that the scale of a predictor affects its magnitude of contribution and that the penalization is correct/fair, we should thus standardize the variance of all predictors to one."
```
### Task 1d)
Now we are in the position to write a function that estimates the ridge regression coefficient estimates for us. Write such a function `beta.ridge` using matrix operations in R. The function inputs should have predictors `X`, outcome `y` and the value of the tuning parameter `lambda` as inputs. 
```{r}
beta.ridge <- function(X, y,lambda){
  I <- diag(length(diag(X)))
  beta <- solve(t(X)%*%X+lambda*I)%*%t(X)%*%y
  return(beta)
}

```
### Task 1e)
In order to measure model generality, we also want to obtain the degrees of freedom of a particular ridge regression with shrinkage parameter $\lambda$. 
Write the function `dfs.ridge` using `X` and `lambda.seq` that derives these degrees of freedom for the ridge problem for a sequence of $\lambda$'s.
```{r }
dfs.ridge <- function(X, lambda.seq){
  n.lambda <- length(lambda.seq)
  dfs <- rep(0, n.lambda)
  I <- diag(length(diag(X)))
  for(i in 1:n.lambda){
   dfs[i] <- sum(diag(X%*%solve(t(X)%*%X + lambda.seq[i]*I)%*%t(X)))
 }
  return(dfs)
}

```
### Task 1f)
What does regularization, expressed by the value of the tuning parameter $\lambda$ to the estimated coefficients of ridge regression. The functions above allow you to investigate this question and to answer the following questions:

1. What is the relation between $RSS$ and $\lambda$ and why is it that way?
2. Howw does that show that we can't use the training error for selecting the hyperparameters?

``` {r }
lambda.grid <- 10^(seq(0,3, length=100))
all.beta <- matrix(NA, nrow=dim(X)[2], ncol=length(lambda.grid))
all.dfs  <- dfs.ridge(X, lambda.grid)
all.RSS  <- all.dfs
for (i in 1:length(lambda.grid)) {
  all.beta[,i] = beta.ridge(X, y, lambda.grid[i])
  all.RSS[i] = mean((y-X%*%all.beta[,i])^2)
}
library(ggplot2)
library(reshape2)
fig1 <- ggplot(data.frame(lambda = lambda.grid,
                  dfs = all.dfs),aes(x=lambda, y=dfs)) + 
  geom_line() +
  scale_x_log10()
print(fig1)
fig2 <- ggplot(data.frame(lambda = lambda.grid,
                  RSS = all.RSS),aes(x=lambda, y=RSS)) + 
  geom_line() +
  scale_x_log10()
print(fig2)
rownames(all.beta) <-colnames(X)
colnames(all.beta) <- lambda.grid
all.beta.long = melt(all.beta, id = variable)
names(all.beta.long) <- c("Variable", "lambda", "Coefficient_value")
fig3 <- ggplot(all.beta.long, aes(x=lambda)) + 
  geom_line(aes(y=Coefficient_value, color=Variable)) + 
  scale_x_log10()
ex1f_relation_between_RSS_lambda <- "(1)The relationship between RSS and lambda is positive, meaning that the greater the lambda, the bigger the RSS. This is due to the decreasing flexibility of the ridge regression fit with an increasing lambda, since the coefficients are more shrunk towards zero due to the greater magnitude of the penalty (as dictated by lambda). Thus, an increasing lamdba results in increased bias and decreased variance, leading to a higher RSS than with a comparably smaller lambda. Intuitively, this makes sense because lambda equals zero is the OLS regression, which gives a smaller RSS than ridge or lasso, since the minimization problem doesn't involve a penalty term (see Review Questions 4, Question 8). (2) If we chose hyperparameters based on training error, we would choose a too flexible model (overfitting), that describes the training data too well (small RSS on training data), but might produce a relatively high test error. Thus, we need to add optimism of training error, in order to prevent a too optimistic training error. Both the degrees of freedom and RSS of the training data would suggest a lamdba close to or of zero, whereas through e.g. cross-validation, we would find a very different 'optimal' lamdba."
```
### Task 1g)
Below we plot what is known as the ridge path. Explain what the figure displays.
```{r }
rownames(all.beta) <-colnames(X)
colnames(all.beta) <- lambda.grid
all.beta.long = melt(all.beta, id = variable)
names(all.beta.long) <- c("Variable", "lambda", "Coefficient_value")
fig <- ggplot(all.beta.long, aes(x=lambda)) + 
  geom_line(aes(y=Coefficient_value, color=Variable)) + 
  scale_x_log10()
print(fig)
ex1g_fig_what_do_I_see <- "The figure shows that an increasing lambda pulls the coefficients towards zero. A greater magnitude of the penalty term (higher lambda) shrinks the coefficents, which makes the fitting less general."
```

### Task 1h)

We have discussed quite extensively that increasing the value of the shrinkage parameter $\lambda$ will draw the vector of estimated slope coefficients further to zero. However, when I have a look at the ridge path in Task 1g, I see at least one slope coefficient whose values temporarily *increases* as we increase $\lambda$. How is this possible?

``` {r }

explain_ridge_path_mystery1h <- "It is true that an increasing lambda minimizes the coefficients and pulls them towards zero, however, not the coefficients seperately, but rather the squared sum of coefficients (the whole term). Thus, the sum of squares is further minimized with an increasing lambda, which may result in some coefficients temporarily increasing, while the sum continuously decreases."

```


### Task 1i)
So far we have only looked at the training error (RSS). However, using this measure to choose our tuning parameter value will surely lead to overfitting.  As a remedy, we will add the optimism of training error, $\omega$, in order to arrive at an estimate of in-sample error, $E_y[Err_in] $. In Lecture 5, we encountered this estimate as *Mallow's $C_p$*. Since ridge regression requires us to specify a tuning parameter, $\widehat{E_y[Err_{in}]}(\lambda)$ is a function of $\lambda$.
Write a function that for each $\lambda$ in `lambda.seq` computes estimated $\widehat{E_y[Err_{in}]}(\lambda)$ (hint the formula is in the lecture slides), and use $\hat{\sigma}^2 = \frac{RSS}{n}$  as an estimate of $\sigma^2$.
```{r }
err.ridge <- function(X, y, lambda.seq){
  n.lambda <- length(lambda.seq)
  err <- rep(0, n.lambda)
  N <- length(y)
  beta1 <- matrix(NA, nrow=dim(X)[2], ncol= length(lambda.seq))
  dfs <- dfs.ridge(X, lambda.seq)
  rss <- dfs
  ols <- lm(lpsa~lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45 -1, data=prostate.train)
  res <- residuals(ols)
  sigma1 <- var(res)
    for(i in 1:n.lambda){
      dfs <- dfs.ridge(X,lambda.seq[i])
      beta1[,i] <- beta.ridge(X, y, lambda.seq[i])
      rss <- sum((y-X %*%beta1[,i])^2)
      err[i] <- ((1/N) * rss) + ((2/N) * sigma1 * dfs)
  }
  return(err)
}

```
### Task 1j)
Use the function `err.ridge` to compute the estimated in-sample error as a function of degrees of freedom. Here we can use the estimated in-sample error for estimating the hyperparameter. From the $\widehat{E_y[Err_{in}]}(\lambda)$  select $\lambda$  in  `lambda.est`
```{r }
lambda.seq <- seq(0,5,length.out = 100)
err <- err.ridge(X,y, lambda.seq)
fig <- ggplot(data.frame(err=err, lambda=lambda.seq), aes(x=lambda)) + 
  geom_line(aes(y=err)) 
print(fig)
lambda.est <- lambda.seq[which.min(err)]
```


## Part two: glmnet
Now that you implemented your own ridge regression we are going learn how one can fit it in R using `glmnet`, the go-to library for lasso and ridge. In this part, you will fit both ridge and lasso on the data and familiarize yourself with the various functions in `glmnet`.
First we load the library
```{r  }
library(glmnet)
```
### Task 2a)
Fit ridge regression in `glmnet` for an equally spaced grid of 400 tuning parameter values $\lambda$ between 0 and 20 .
To do this, read the help section for `glmnet` and estimate the ridge.regression model with 
```{r  }
lambda.seq <- seq(from=0, to=20, length.out=400)
glm.obj <- glmnet(X, y, alpha = 0, lambda = lambda.seq)
plot(glm.obj, xvar='lambda')
```

### Task 2b)
In `glmnet` one often fits the parameters using n-fold cross-valdiation. This is done by the function `cv.glmnet`. Here you should find a $\lambda$ for the ridge regression problem through 10-fold cross-valdiation with mean squared error as loss function. Read the first five pages of the introduction [https://rdrr.io/cran/glmnet/f/inst/doc/glmnet.pdf](intro glm). Also note that in glmnet one is choosing the $\lambda$ that minimizes $\frac{1}{2N} RSS + \frac{\lambda}{2}||\beta||_2^2$ which means that $\lambda$ here corresponds to $\frac{\lambda}{n}$ in task one.
For `lambda.seq` use 100 equally spaced values between 0 and 2.
```{r }
set.seed(5)
lambda.seq <- seq(0,2, length.out = 100)
glm.cv.obj.ridge <- cv.glmnet(X, y, alpha=0, lambda = lambda.seq, type.measure = "mse",intercept=FALSE)
plot(glm.cv.obj.ridge)
ridge.lambda.min <- glm.cv.obj.ridge$lambda.min
ridge.coeff.lambda.min <- coef(glm.cv.obj.ridge, s = "lambda.min")
fig2c_what_does_the_horzontal_line_mean<- "The vertical lines (black dotted) along the cross-validation curve indicate two values: (1) lambda.min, which is the value of lambda that gives the mean cross-validated error (2) lambda.1se, which is the value of lambda that gives the most regularized model, so that the cross-validated error is within one standard error of the minimum."  
fig2c_what_does_the_bars_mean<- "The bars error bars, indicating the the upper and lower standard deviation curves along the lambda sequence."  

```
### Task 2d)
It is beyond the scope of this course to derive a solver for the lasso problem. But one can again use  `glmnet` to find the parameters.
We will go through the same step for the ridge regression but for lasso. First we fit the parameters on the full training data.

```{r }
glm.obj.lasso <- glmnet(X, y, alpha=1 , lambda = lambda.seq)
plot(glm.obj.lasso, xvar='lambda')
```
### Task 2e) 
To find the hyperparameter, $\lambda$, we again use 10-fold cross-validation with the help of `cv.glmnet` .
```{r }
set.seed(5)
glm.cv.obj.lasso <- cv.glmnet(X, y, alpha=1, lambda = lambda.seq, type.measure = "mse",intercept=FALSE)
plot(glm.cv.obj.lasso)
lasso.lambda.min <- glm.cv.obj.lasso$lambda.min
lasso.coeff.lambda.min <- coef(glm.cv.obj.lasso, s = "lambda.min")
lasso.coeff.lambda.se <- coef(glm.cv.obj.lasso, s = "lambda.1se")
```
## Final part: model performance
### Task 3a) 
Now we are going to see how well our models perform on new data (i.e. test data). However, first we are going to center and scale the data using our old object `scaled.param`. The reason we don't scale with new data is that one does not have this information when doing the prediction (you can only demean data that you observed), if we would have done this we would in some sense be cheating.
```{r  }
prostate.test <- prostate[train_index==F,]
prostate.test <- predict(scaled.param, prostate.test)
 X.test <- model.matrix(lpsa~lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45 -1, data=prostate.test)
 y.test <- prostate.test$lpsa
  
```
### Task 3b) 
Now you are going to compare the mean RSS for the model we fitted above. This we will do by using the predict function `predict` which you can read about with `help("predict.glmnet")`. Which model fitting method preformed best?
``` {r }
beta_ols <- solve(t(X)%*%X, t(X)%*%y)
yhat_ols <- X.test%*%beta_ols
yhat_ridge.min <- predict(glm.cv.obj.ridge, newx=X.test, s="lambda.min")
yhat_lasso.min <- predict(glm.cv.obj.lasso, newx=X.test, s="lambda.min" )
yhat_lasso.se <- predict(glm.cv.obj.lasso, newx=X.test, s="lambda.1se" )
RSS.ols <- mean((y.test - yhat_ols)^2)
RSS.lasso.min <-  mean((y.test-yhat_lasso.min)^2)
RSS.ridge.min <-  mean((y.test-yhat_ridge.min)^2)
RSS.lasso.se <-  mean((y.test-yhat_lasso.se)^2)
Which_was_the_best <- "The Lasso one standard error (RSS.lasso.se) method performed best with a RSS of 0.317, whereas lasso and ridge produced a RSS of 0.327 and 0.338, respectively."
```