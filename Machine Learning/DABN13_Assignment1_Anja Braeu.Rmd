---
title: "Assignment 1"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

This lab comes in the form of an R Markdown document which you are supposed to fill in. All instances that require you input are marked by "??". Please replace that with the corresponding code for a given task. Additionally, you need to uncomment all commented (`#`) lines in the R code chunks below in order to make the script work. Moreover, note the following:

* Often, we have specified names for objects that you are supposed to create. Use them. Do not come up with any object names of your own.
* Tasks that require you to write a function will provide you with function name, all inputs to the function as well as the function output. Your task is to write operations within the function that work with these given parameters. 
* At times, you will be asked to use a specific function to complete a task. If this is the case, please follow instructions to the point. Do not implement any alternative  way of performing the task.
* Sometimes, you might have questions concerning the use of a specific R command. Please approach this situation as in a real-life programming situation: First, use the R help files to find an answer. If unsuccessful, use Google. If still unsuccessful, post your question on Slack.
* Please write text answers into the corresponding string variables.


## Part one: Least squares regression mechanics

In this section, you are supposed to create your own algorithms for some standard statistics of a fitted linear regression model from scratch. You are only allowed to use the operations
```{r ,echo=FALSE}
#cat("solve, %*%, /, \n")
```
to manually produce results otherwise produced by the lm() command.

# Load the data set

We will be working with ` Guns.dta`, a Stata dataset containing yearly US state data between 1977 and 1999 of three different crime rates, a number of additional state characteristics, as well as an indicator for the existence of a "shall-carry" law that allows citizens to obtain a permission to wear concealed handguns. In the following, you will fit a simple predictive model for state-wide violent crime rates.

To begin with, use the read.dta command in the "foreign" package to load ` Guns.dta`


```{r , echo=T}
library(foreign)
library(tidyverse)
setwd("C:/Users/anjab/Desktop/DABE 202122/1_Machine Learning/Assignments")
guns.data <- read.dta("Guns-1.dta")

```


# Task 1a)
First, use the names()-command to report the variable names of `guns.data`
Then, construct an ` X`-matrix containing the columns for an intercept, the logarithm of state population, average per capita income, shall-carry law in effect, as well as the logarithmic rates for murder and robberies. Additionally, create a `y`-vector containing the log violent crime rate (for the state that year).
```{r , echo=T}
names(guns.data)

guns.data <- mutate(guns.data, 
         intercept = 1,
         log.pop = log(pop),
         log.mur = log(mur),
         log.rob = log(rob),
         log.vio = log(vio))
X <- cbind(guns.data$intercept, 
           guns.data$log.pop, 
           guns.data$avginc, 
           guns.data$shall, 
           guns.data$log.mur, 
           guns.data$log.rob)
y <- guns.data$log.vio

```

# Task 1b)
Build a function that uses ` X` and ` y` as inputs and returns the least squares
estimate $\hat{\beta}$ of the slope coefficients on ` X`. Here you are only allowed to use matrix and scalar operations.
```{r , echo=T}
estimate.beta <- function(X, y){
  beta = solve(t(X) %*% X) %*% t(X) %*% y
return(beta)
}
```

# Task 1c)
Build a function that computes the model residuals. Refer to the previous function `estimate.beta` to get an estimate of the slope coefficients. Here you are only allowed to use matrix and scalar operations.
```{r , echo=T}
estimate.residual <- function(X, y){
  res = y-X%*%estimate.beta(X,y)
  return(res)
}

```

# Task 1d) 
Build a function that computes $R^2$, i.e. the estimated proportion of variance of $Y$ that is explained by the covariates in your model. Refer to `estimate.residual` to get model residuals. Here you are only allowed to use matrix and scalar operations.
```{r , echo=T}
estimate.R2 <- function(X, y){
  RSS <- t(estimate.residual(X,y))%*%estimate.residual(X,y)
  TSS <- t(y-mean(y))%*%(y-mean(y))
  R2 <- 1 - (RSS/TSS)
return(R2)
}

```

## Part two: Linear regression practice

# Task 2a)
Now use the lm()-command to fit the same regression model as in Task 1. Refer to the ` guns.data` dataset directly instead of using the matrices ` X` and ` y`.
```{r, echo=TRUE}
lm.fit2a <-  lm(log.vio ~ log.pop + avginc + shall + log.mur + log.rob, guns.data)

```

# Task 2b)
Least squares regression coefficients can be extracted from the fitted model ` lm.fit2a ` by using the `coef()`-command. Save the coefficients as a new object. Use your function from task 1b) to get manually constructed least squares estimates. Then, calculate the sum of squared differences between the elements of the two coefficient vectors to confirm that they are practically identical.

```{r, echo=TRUE}
lm.coef2b     <- coef(lm.fit2a)
manual.coef1b <- estimate.beta(X,y) 
diff.beta2b     <- sum((lm.coef2b-manual.coef1b)^2)
print(diff.beta2b)

```  

# Task 2c)
Model residuals can be extracted from objects created by `lm` using the `residuals()` function. Obtain the model residuals of the regression from task 2a in this way. Additionally, residuals are saved inside the ` lm.coef2b` object. Report the names of all objects within ` lm.coef2b` and calculate the sum of squared differences between the residuals you find there and the residuals that you extracted using `residuals()`.

``` {r, echo=TRUE}
lm.res2c <- residuals(lm.fit2a)
names(lm.coef2b)

diff.res <- sum((lm.res2c-lm.fit2a$residuals)^2)
print(diff.res)

```

# Task 2d)
In order to obtain fitted values, we can use the `predict()`. Do this. The data for which we predict here is the same data used for model training. Accordingly, only need to specify one argument (i.e. input) for `predict()`. 

```{r, echo=TRUE}
lm.pred2d = predict(lm.fit2a)

```

# Task 2e)
A good prediction model for violent crime rates should capture all systematic patterns in the variation of this variable. A simple, but very effective way of finding out whether this is the case is to look at residual plots. If model residuals look like more than just pure noise, then there must be patterns left that we can exploit. Begin by plotting the model residuals from Task 2c (y-axis) against the fitted values from Task 2d (x-axis). Are there remaining patterns in the data?

``` {r, echo=TRUE}
library(ggplot2)
figure2e <- ggplot() +         # opens plot surface
              geom_point(aes(y=lm.res2c, x=lm.pred2d)) + # adds scatter plot
              geom_smooth(aes(y=lm.res2c, x=lm.pred2d), formula = y~x, se=FALSE,method='loess', col='red') # adds "a fitted smooth curve"
print(figure2e)
rem_patterns2e <- "The residual plot looks like pure noise and shows no remaining pattern in the variation of the variable for violent crime rates." #Write answer as string variable 

```


# Task 2f)
Let us proceed with another plot that should highlight an obvious source of unaccounted patterns in the data. Plot the model residuals against ` stateid`. What do you see?

``` {r, echo=TRUE}

figure2f <- ggplot() + 
              geom_point(aes(y=lm.res2c, x=guns.data$stateid))
print(figure2f)

whatIsee2f <- "The StateID seems to account for some patterns in the data, as the residual plot shows (note: Residual= Observed - Predicted value). In some states, the observed violent crime rate is higher than predicted, while in other (e.g. stateid=14), the predicted value is much higher than the actual crime rate. To account for those unobserved patterns and avoid omitted variable bias, 'stateid' should be included in the regression."


```


# Task 2g) 
`stateid` is a variable that want to add to our model specification in some form. Before doing this, use the `summary()` command to get some descriptive statistics this variable in ` guns.data`. You will see that a mean and a median are reported. Hence, as what type of variable is `stateid` apparently seen by R? Would it make sense to add this variable into our model from Task 2a) as it currently is? Why or why not?

```{r, echo=TRUE}
summary2g <- summary(guns.data$stateid)
print(summary2g)

typeofvarb2g <- "Accoding to R, the variable stateid belongs to a numeric class. The command 'typeof(guns.data$stateid)' reveals that R sees the variable stateid as an integer. Theorethically, it would be possible to perform arithmetic operations on this variable, however, it would make no sense (see below). "
in_regmodel2g <- "No, it would make no sense to add stateid as a numeric variable to our regression model, since it is a factor, often referred to as categorial variable. Factors are often stored as a vector containing integer values, each number having a corresponding character value. Hence, each number denominates a state and should be treated as a factor in a regression model."

```

# Task 2h)
The way in which R treats a specific variable can change considerably if we encode it as a factor variable. Hence, replace the variable `stateid` in `guns.data` with a version if itself that is encoded as factor variable. Use the `factor()` command for that. Next, get the summary statistics of this modified variable. What has changed?

```{r, echo=TRUE}
guns.data$stateid <- factor(guns.data$stateid)
summary2h <- summary(guns.data$stateid)
print(summary2h)
whatchanged2h <- "After encoding stateid as a factor, the summary statistics don't show the median, mean and other summary statistics of an integer, but merely print the ids of all states. While the integers denominating each state remain, the class of the variable was changed from integer to factor."


```

# Task 2i)
Estimate the regression model from Task 2a with factor variable `state_id` as an additional regressor. Use the `summary()` command to report a summary of the regression results. How has `lm()` included `stateid` into the model?

```{r, echo=TRUE}
lm.fit2i <- lm.fit2a <-  lm(log.vio ~ log.pop + avginc + shall + log.mur + log.rob + stateid, guns.data)
summary2i <- summary(lm.fit2i)
print(summary2i$coefficients[1:15,])

howincluded2i <- "The lm() command includes each level of the factor stateid seperately in the regression, since it automatically transforms it into numerical variables before fitting the model, without explicitly creating them in the dataset."

```

# Task 2j)
The regression results in Task 2i) look the way they do because the ` lm()` command conveniently transforms the factor variable `stateid` into numerical variables before fitting the model. In particular, the `model.matrix()` command is automatically used to arrive at a set of regressors that one can directly feed into a least squares estimation routine. Some important R-commands are less convenient and require you to transform the predictors yourselves. In order to prepare for this situation, use the `model.matrix()` command manually with the same model specification as in Task 2i to get the set of predictors internally generated by `lm()`. Inspect the resulting matrix (e.g. using the `View()` command)  and describe in how far it differs from the variables that you specified.
```{r, echo=TRUE}
predictors2j   <- model.matrix(lm(log.vio ~ log.pop + avginc + shall + log.mur + log.rob  + stateid, guns.data))
howxmatdiffers2j <- "What lm() did implicity (i.e. create dummy variables from each level of stateid), the model.matrix() command does explicitly. While the other predictors remain unchanged, dummy (0/1) variables for each level of stateid were created."

```

# Task 2k)
The set of predictors created in Task 2j allows you to use the set of functions for fitting a linear regression model that you wrote in Part 1 of this assignment. We will confirm this by using the ` estimate.R2` function written in Task 1c. Use the matrices ` y` and `predictors 2j` to obtain the R2 of the model specification of Tasks 2i-j. Additionally, use the matrices `y` and ` X` to get an R2 for the model in Task 2a. How did inclusion of `stateid` affect the capability of a linear regression to explain variation in violent crime rates in the sample used for fitting the model?

```{r, echo=TRUE}
lm.R2_withstate2k   <- estimate.R2(predictors2j,y)
lm.R2_nostate2k     <- estimate.R2(X,y)
print(c(lm.R2_withstate2k,lm.R2_nostate2k ))
effect_of_stateid2k <- "The linear regression model excluding 'stateid' explains about 85% (R2=0.85) of the variation in violent crime rates, whereas the model including 'stateid' explains 96% (R2=0.96) of the variation. Thus, including 'stateid' enhanced the capability of the linear regression model to explain the variation in violent crime rates."

```